---
layout: post
title: "cs224n: Fancy RNNs: LSTM, GRU, ..."
author: "Yi"
header-style: text
mathjax: true
tags:
  - nlp
---

Notes for [lecture 07](http://web.stanford.edu/class/cs224n/slides/cs224n-2020-lecture07-fancy-rnn.pdf). In Vanilla RNN, the hidden state is constantly being overwritten, is there a
way to preserve the hidden states in a separate memory and reuse it later?

##  Long Short-Term RNN

On step  $\displaystyle t$  , there is a hidden state  $\displaystyle h^{(t)}$
and cell state  $\displaystyle c^{(t)}$

  * Both are vectors with length  $\displaystyle n$  . 
  * The cells store long term information. 
  * The LSTM can erase, write, read information from the cell state. 

The selection of which information to erase/write/read is controlled by 3
gates:

  * The gates are also vectors with length  $\displaystyle n$  . 
  * On each timestamp, open gate  $\displaystyle \rightarrow 1$  , close gate  $\displaystyle \rightarrow 0$  . If the gate is open, then information can be passed through. 
  * The gats are dynamic, which means they are not the same for all timestamps, the values are computed on the current context. 

For each hidden state, we have 3 gates: forget gate, input gate, and output
gate. Forget gate  $\displaystyle f^{(t)}$  works as the weight for
$\displaystyle c_{t-1}$  to decide whether to keep or forget last cell state.
Input gate  $\displaystyle i^{(t)}$  works as the weight for  $\displaystyle
\tilde c_t$  . Output gate  $\displaystyle o^{(t)}$  works as the weight for
$\displaystyle tanh(c^{(t)})$  to give the output.

  * $\displaystyle f^{(t)} = \sigma(W_fh^{(t-1)} + U_fx^{t} + b_f)$ 
  * $\displaystyle i^{(t)} = \sigma(W_ih^{(t-1)} + U_ix^{t} + b_i)$ 
  * $\displaystyle o^{(t)} = \sigma(W_oh^{(t-1)} + U_ox^{t} + b_o)$ 

From the gate equation, we can see that LSTM have 3 groups of hidden states
compare with Vanilla RNN. When computing the output, we apply  $\displaystyle
tanh$  function:

  * $\displaystyle \tilde c^{(t)} = tanh(W_ch^{(t-1)}+U_cx^{(t-1)} + b_c)$ 
  * $\displaystyle c^t = f^{t}\circ c^{(t-1)} + i^{t}\circ \tilde c^{t}$ 
  * $\displaystyle h^{t} = o^t\circ tanh(c^{t})$ 

The tuition of using  $\displaystyle tanh$  is that sigmoid function's range
$\displaystyle (0, 1)$  ,  $\displaystyle tanh$  function's range is
$\displaystyle (-1, 1)$  , use  $\displaystyle tanh$  for cell state can let
it show negative effect on the hidden state.

![](/img/in_post/v2-3aa9df4f94712b58da204486a9f3a0f8_b.jpg)

The prediction could be calculated in the same way as vanilla RNN:
$\displaystyle \hat y = softmax(Uh^{(t)}+b_y) \in \mathbb{R}^{|V|}$

After 2019, Transformer is the mainly used technique for certain tasks.

##  Gate Recurrent Units (GRU)

A simpler alternative of LSTM.

We don't have a cell state in GRU. Then how to access information from
timestamp many steps back? We introduce 2 gates: update gate and reset gate.

Update gate is a weight applies to the current estimated hidden state
$\displaystyle \tilde h^{t}$  i.e.  $\displaystyle h^{(t)} = (1-u^t)\circ
h^{(t-1)} + u^t\circ \tilde h^{(t)}$  . It decides the importance of an
estimated hidden state(which is computed in a similar way as vanilla RNN with
an additional reset gate.) and previous state on the hidden state.

Reset gate is used in the estimated hidden state:  $\displaystyle \tilde h^{t}
= \sigma(W_h u\circ h^{(t-1))} +U_hx^{t} + b_k)$  , it pairwise product with
the previous hidden state.

Why does it help the vanishing gradient issue? Imagine that  $\displaystyle
u^t = 0$  , then the current hidden state will be the same as the previous
hidden state.

  

How to choose between LSTM and GRU? A rule of thumb is choosing LSTM as
default, if there is an improvement, then test on GRU too. GRU has fewer
parameters thus easier to train. LSTM performs better when sentence length is
long since it has more parameters and can capture information from previous
timestamps slightly better.

##  Is Vanishing Gradient just an RNN Problem?

No. It happens on feed-forward DNN and Convolutional NN as well, especially
for deep ones. That means the training would be very slow for bottom layers.

Solution: Add more direct connections, thus allowing the gradient to flow.

e.g.

  1. ResNet, aka residual connections or skip connections. Shown in this figure, the identity connection preserves information by default. This makes DNN much easier to learn. 

![](/img/in_post/v2-1d47adfa4ca7c588fbeff6c75af308ce_b.jpg)

2\. Dense connections aka [DenseNet](https://arxiv.org/pdf/1608.06993.pdf), connecting everything to everything.

![](/img/in_post/v2-6c2ad930a391352600f8b6f164032f9e_b.jpg)

3\. Highway connections aka "HighwayNet". It is similar to ResNet, but replace
the identity connection with a dynamic gate control(inspired by the idea of
LSTM, but apply to feed-forward NN and convolutional NN).

Conclusion: Although vanishing/exploding gradients are a general problem,
**RNNs are particularly unstable** because of the repeated multiplication by
the **same** weight matrix.

##  Bidirectional RNN

![](/img/in_post/v2-79350efbba54895f41b3bfdbc3e53045_b.jpg)

The forward RNN and backward RNN use different weights. This allows the
contextual representation has both left and right context.

###  Bidirectional RNN is only applicable when you have access to the entire
sentence.

So for the language model, we cannot use Bidirectional RNN to predict the next
word, because we only have left context available.

e.g. BERT(Bidirectional Encoder Representation from Transformers) is a
powerful pretrained contextual representation system built on
bidirectionality.

##  Muti-layer RNN

The motivation is that the lower RNN should learn the lower-level features,
and the higher RNN should learn the higher-level features. The computation
order is flexible, you could do one layer at one time, and then the next
layer.

![](/img/in_post/v2-c16b62a349eb25485e8871df816dc232_b.jpg)

