---
layout: post
title: "cs224n: Language Model and RNN"
comments: true
author: "Yi"
header-style: text
mathjax: true
tags:
  - nlp
---

Notes for [lecture 06](http://web.stanford.edu/class/cs224n/slides/cs224n-2020-lecture06-rnnlm.pdf).

##  Language Model

language model is the task of predicting what word comes **next** . It could
be regarded as a kind of classification task.

  * Input: text  $\displaystyle x^{(1)}, x^{(2)}, ..., x^{(T)}$ 
  * Output: then the probability of this text  $\displaystyle P(x^{(1)}, x^{(2)}, ..., x^{(n)}) = \prod^n_{t = 1}(P(x^{(t)}|x^{(t-1)}, x^{(t-2)}, ..., x^{(1)}))$ 

Importance:

  * Language Model is a benchmark that helps us measure the progress on language understanding. 
  * Language model is a subcomponent of many NLP task. e.g. predictive typing, speech recognition, handwriting recognition, spelling/grammar correction. 

###  N-Gram Language Model

n-gram: a chunk of consecutive n words.

assumption:  $\displaystyle x^{(t)}$  only depends on the previous n words:
$\displaystyle x^{(t-1)}, x^{(t-2)}, ..., x^{(t-n+1)}$  '

The conditional probability:

$\displaystyle P(x^{(t)}|x^{(t-1)}, x^{(t-2)}, ..., x^{(t-n+1)}) =
\frac{P(x^{(t)}, x^{(t-1)}, x^{(t-2)}, ..., x^{(t-n)})}{P(x^{(t-1)},
x^{(t-2)}, ..., x^{(t-n+1)}))}\approx \frac{count(x^{(t)}, x^{(t-1)},
x^{(t-2)}, ..., x^{(t-n+1)})}{count(x^{(t-1)}, x^{(t-2)}, ..., x^{(t-n+1)}))}$

Issue:

1) How to choose n? Increasing n will cause more sparsity problem.

2) Sparsity problem1: For not shown words, is the probability 0? Solution: Add
a small delta

3) Sparsity problem2: What if the denominator is 0? Never shown context.
Partial Solution: Decrease n.

Sparsity problem: Since the parameters are trained with very few samples, it
would be pretty noisy.

###  A Neural Network with N-Gram

![](/img/in_post/v2-ff55df8a8e27f15c7e5ca51aee3936eb_b.jpg)

Issues:

1) W will grow with window size!

2) Not efficient because different word positions doesn't share parameters in
W with each other, then the model may learn some duplicate processing logic
for different positions.

###  RNN(Recurrent Neural Networks)

Motivation: We need a NN that can process any length input.

In RNN we introduce hidden states, for each word it has a hidden state that
depends on the hidden state of the previous word and the input features. The
reason that we call it hidden states since it is a state that mutating for
different positions, different versions for a same thing. One important thing
is that the weights  $\displaystyle W$  for different inputs are the same. The
number of hidden states is the same as input number.

![](/img/in_post/v2-c7f4642ed3ca4ebdf5b522ed57a49b6c_b.jpg)

For the first hidden state, we may put a random initialized or pretrained
state  $\displaystyle h^{(0)}$  .

The prediction  $\displaystyle y^{(t)} = softmax(Uh^{(t)} + b_2)$  , where
$\displaystyle h^{(t)} = \sigma(W_hh^{(t)}+W_eE(x^{(t)}) + b_1)$  .
$\displaystyle x^{(t)}$  is one-hot vector and  $\displaystyle E(X^{(t)})$  is
the word embedding.

The loss function should be  $\displaystyle J(\theta) =
\frac{\sum_{i=1}^T(J(\theta^{(i)}))}{T}$  , but it is too expensive to compute
this loss for the whole corpus considering that the length could be very long.
So we can process a batch of sentences instead, update the weights, and
repeat.

Advantages of RNN:

  1. It can process any length of text 
  2. Theoretically the next output can learn information from many steps back 
  3. Model size doesn't grow since what we need is always  $\displaystyle W$  . 
  4. Since we use the same weights on all inputs, there is symmetry on how inputs are processed. 

Disadvantage of RNN:

  1. Slow computing because of sequential training 
  2. In practice, it's hard to access information many steps back. 

###  Evaluation

We cannot just use the generated text as an evaluation metric, we do need some
measurable metrics.

The standard evaluation metric for Language Models is perplexity.

$\displaystyle Perplexity = \prod^T_{t=1}(\frac{1}{P_{LM}(x^{(t+1)}|x^{(t-1)},
x^{(t-2)}), ..., x^{(1)})})^{\frac{1}{T}}$

We need to normalize the product with  $\displaystyle \frac{1}{T}$  where
$\displaystyle T$  is the corpus number. Since  $\displaystyle J(\theta)
=\frac{1}{T}\sum_{t=1}^{T}(-log(y^{(t)}_{x_{t+1}})$  ,  $\displaystyle
Perplexity = exp(-J(\theta))$

So the lower the  $\displaystyle Perplexity$  , the better the model
performance.

The RNN introduced in this lecture is called vanilla RNN, because there are
more complex RNNs like GRU, LSTM and multi-layer RNN.

###  Issue of Vanilla RNN

###  Vanishing/Exploding gradient

Consider the computing function provided in Lecture 6  $\displaystyle h^{(t)}
=\sigma(\textbf{W}_hh^{(t-1)} + \textbf{W}_x\textbf{x}^{(t)} + b_1)$  .

Therefore  $\displaystyle \frac{\partial h^{(t)}}{\partial h^{(t-1)}}
=W_h\cdot diag(\sigma'(h^{(t)}))$  . The magnitude of  $\displaystyle
diag(\sigma'(h^{(t)}))$  won't be larger than 1 due to the sigmoid function
property.

Gradient  $\displaystyle \frac{\partial J(\theta)^i}{\partial h^{(j)}}=
\frac{\partial J(\theta)^i}{\partial h^{(i)}}\cdot\frac{\partial
h^{(j)}}{\partial h^{(i-1)}} \cdot \frac{\partial h^{(i-1)}}{\partial
h^{(i-2)}}\cdot ...\frac{\partial h^{j+1}}{\partial h^{(j)}}$

Then if  $\displaystyle ||W_h|| < 1$  (largest eigenvalue is larger than 1),
the gradient would be vanishing when propagating back as  $\displaystyle
||W_h||^{(i-j)}$  will exponentially decreasing to 0. Similarly, if
$\displaystyle ||W_h|| > 1$  , gradient would be exploding.

![](/img/in_post/v2-d0fdb1ca46e2eac59f9ce492724022b7_b.jpg)

We see the short-term gradient is not comparable with long-term gradient. For
vanishing gradients, we may lose connection between inputs that far away. For
exploding gradients, the updating step may become very large, it may exceed
the numerical upper bound(infinity).

We can resolve the second issue by gradient clipping. i.e. If the gradient's
magnitude exceed some certain number, we scale it down to a range before
applying the SGD update

