---
layout: post
title: "cs224n - Local Context Window Based: Word2Vec"
comments: true
author: "Yi"
header-style: text
mathjax: true
tags:
  - nlp
---


##  Localist Representation - one-hot Vector

Issues:

  1. word is disconnected with its parent words. 
  2. vectors are orthogonal 

##  Solution for Issue 1: Distributional Semantics

How to represent the meaning of a word is by looking at the contexts in which
it appears.

##  Solution for Issue2: Word Embedding

Dense vectors. Dense: all numbers are non-zeros.

##  Word2Vec -> Word Embedding with Distributional Semantics

A framework for learning word vectors to represent words' distributional
semantics. Requirements: large corpus of text (concatenated of all docs
available)

###  Basic Idea

For position  $\displaystyle t$  , it has a center word  $\displaystyle c $
and context words  $\displaystyle o$  . We have 2 ways to define loss function
and train the model

  1. skip-gram: predict context word according to center word. 
  2. CBOW(continuous bag of words): predict center words given a bag of context words. 

For skip-gram:

We want to find an object function to maximize  $\displaystyle P(c|o)$  . Set
the unknown parameter as  $\displaystyle \theta$  , then for each position
$\displaystyle t$  and  $\displaystyle i$  within window  $\displaystyle [-l,
l]$  ,  $\displaystyle P(c|o) = P(w_{t+i}|w_t, \theta)$  .

For the whole corpus, the likelihood is  $\displaystyle L(\theta) =
\prod_{t\in T}^{}\prod_{i=-l, i\ne0}^{l}P(w_{t+i}|w_t)$  .

So the final objective function is  $\displaystyle J(\theta) = -logL(\theta) =
\sum_{t\in T}^{}\sum_{i=-l, i\ne0}^{l}log[P(w_{t+i}|w_t)]$

###  How to define  $\displaystyle P(w_{t+i}|w_t, \theta)$

Set 2 vectors for each word, one for context word and one for center word.
Then  $\displaystyle \theta \in R ^{2dv}$  where d is the dimension for each
vector, v is word number.  $\displaystyle P(o|c) =
\frac{e^{u_o^Tv_c}}{\sum_{w\in V}e^{u_w^Tv_c}}$

Gradient of  $\displaystyle J(\theta )$  still have the sum dividend, which
causes expensive computation. To resolve this issue, we bring negative
sampling method.

###  Negative Sampling with Skip-Gram

Main idea: train binary logistic regressions for a true pain(center word and
word in its context window) versus several noise pairs(the center word with a
random word) rather than the pairs in whole corpus. Then the training goal
becomes maximize the possibility of true pairs and minimize possibility of
noise pairs.

It has the same goal as logistic regression.

###  Recall of Logistic Regression

e.g., for  $\displaystyle X \in R^{3,N}$  each  $\displaystyle x_i \quad i\in
N$  , we have a label  $\displaystyle y \in [0, 1]$  . We create a function
$\displaystyle \sigma(f(x))$  to represent  $\displaystyle P(y|x)$  , and
apply sigmoid function to it to predict y so that when  $\displaystyle f(x)
\to \infty$  ,  $\displaystyle \bar y \to 1$  , when  $\displaystyle f(x) \to
-\infty$  ,  $\displaystyle \bar y \to 0$  :

$\displaystyle \bar{y} = \sigma(f(x)) = \frac{1}{1+e^{-f(x)}}$

For positive samples where y = 1, we want to minimize  $\displaystyle y-\bar
y$  , for negative samples where y = 0, we want to minimize  $\displaystyle
\bar y$  . Then  $\displaystyle L(\bar y) = \prod_{i =1}^{N}\bar y_i^y(1-\bar
y_i)^{1-y_i}$

$\displaystyle J(\bar y) = -logL(\bar y) = \sum_{i=1}^N[y_ilog(\bar
y_i)+(1-y_i)log(1-\bar y_i)]$

$\displaystyle f(x) = WX$  (remind that we need to append a const 1 to X to
get the bias parameter).

Log functions's derivative is not stable when x < 1, but here we ensure that
$\displaystyle 1+e^{-f(x)} > 0 $  .

Back to negative sampling.

$\displaystyle \begin{equation} y(c|o)=\left\\{ \begin{aligned} & 1
&\text{shown in context} \\\ & 0 & \text{word not shown in context} \\\
\end{aligned} \right. \end{equation}$

Same as what we set for logistic regression:

$\displaystyle L(u, v) = \prod_{v \in V}\prod_{u \in \text{shown word} \cup
NEG(w)}p(u_o|v_c)=\sigma(u_o^Tv_c)^{y^{(c|o)}}\cdot[1-\sigma(u_o^Tv_c)]^{1-y(c|o)}$

$\displaystyle J(u, v) = - log(L(u, v))=-\sum_{v \in Vc}\sum_{u \in
\text{Context of
}v}\sum_{neg}\\{ylog[\sigma(u^Tv)]+(1-y)log(1-\sigma(u^Tv))\\}$

Notice that  $\displaystyle 1-\sigma(x) = 1 - \frac{e^x}{1+e^{x}} =
\frac{1}{1+e^{x}} =\sigma(-x)$  Then we can simplify the loss function and get
the equation shown in the slides:
![](/img/in_post/word2vec.jpg)


### Reference
[word2Vec blog](https://www.cnblogs.com/peghoty/p/3857839.html)
