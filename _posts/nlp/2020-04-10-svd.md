---
layout: post
title: "cs224n - Count-Base SVD"
comments: true
author: "Yi"
header-style: text
mathjax: true
tags:
  - nlp
---



When reviewing the word2vec model, the professor went through the traditional
method which was used before neural network came out: the co-occurance
matrix(count-base). This method brings a problem: How to reduce the dimension?

To understand it, we need to review several concepts in Linear Algebra:

##  Background

###  1\. Vector

Euclidean vector is a geometric object with **magnitude(length)** and
**direction** . A vector is what needed to carry from point A to point B. The
numbers comprising the vector are called components.(for points, the numbers
are called coordinates)

For most purposes, points and vectors are essentially the same thing, that is,
a sequence of numbers corresponding to measurements along various dimensions.

###  1.1 Scalar Multiplication

Multiplying a scalar(real number) times a vector means multiplying every
component by that real number to yield a new vector.

###  1.2 Dot Product

An algebra operation that takes 2 equal-length sequences of numbers, returns a
single number. Algebraically, the dot product is the sum of the [ products
](https://link.zhihu.com/?target=https%3A//en.wikipedia.org/wiki/Product_%28mathematics%29)
( of the corresponding entries of the two sequences of numbers. Geometrically,
it is the product of the [ Euclidean magnitudes
](https://link.zhihu.com/?target=https%3A//en.wikipedia.org/wiki/Euclidean_vector%23Length)
(length, norm) of the two vectors and the [ cosine
](https://link.zhihu.com/?target=https%3A//en.wikipedia.org/wiki/Cosine) of
the angle between them.

To decide whether two vector are in the same direction, we calculate their dot
product and compare it to the length multiplication result. If two vector's
dot product equals zero, then they are **orthogonal** to each other.

###  1.3 Orthonormal Vectors

Vectors of unit length that are orthogonal to each other are said to be
orthonormal.

###  1.4 Gram-Schmidt Orthonormalization Process

For a vector space, we can use different basis, some basis are orthonormal,
some are not. But in real application, we prefer orthonormal basis. Gram-
Schmidt orthonormalization process is to change a set of non-orthonormal basis
to orthonormal basis.

###  1.5 orthogonal Matrix

$\displaystyle A^TA = AA^T = I$

###  1.6 Determinant

A function of a square matrix that reduces it to a single number. The
determinant of a matrix A is denoted  $\displaystyle |A|$  or  $\displaystyle
det(A)$  .

For 2d matrix, determinant is the area created by the 2 row vectors in A.

###  1.7 Eigenvectors and Eigenvalues

An eigenvector is a nonzero vector that satisfied the equation  $\displaystyle
A\vec{v} = \lambda \vec{v}$  , where  $\displaystyle A $  is a square matrix,
$\displaystyle \lambda$  is a scalar, and  $\displaystyle \vec{v}$  is the
eigenvector.  $\displaystyle \lambda$  is called an eigenvalue.

If we regards square matrix(size: n by n) as a linear transformation function:
$\displaystyle R^{n} \rightarrow R^{n} $  , then any vector  $\displaystyle
\vec{x}$  could be mapped to a new vector  $\displaystyle \vec{x'}= A\vec{x}$
. For most vectors, after this mapping process, its direction will be changed,
but for eigenvectors, the direction will be unchanged.  $^{[1]}$

The advantage of eigenvectors is that if we want to do 100 times
transformation, we don't have to calculate with the matrix for 100 times, we
only need scalar multiplication with  $\displaystyle \lambda^{100}$  . For
other vectors, we can still simplify the linear transformation computation
with eigenvectors. e.g.  $\displaystyle w = t_1v_1 + t_2v_2$  $^{[1]}$

For n by n square matrix, if we get n eigenvalues, then  $\displaystyle A =
W\sum_{}^{}{}W^T$  , where  $\displaystyle W$  consists of the n eigenvectors,
and  $\displaystyle \sum_{}^{}{}$  is a diagonal matrix with n eigenvalues. We
often convert  $\displaystyle W$  to orthonormal vectors, where
$\displaystyle ||w_i||_2 = 1$  , and the n eigenvectors of W has the property
$\displaystyle W^TW=I$  .

When the matrix is rectangular, we use SVD to do the decomposition.

##  Singular Value Decomposition  $^{[2]}$

SVD can be looked at from three mutually compatible points of view.

  1. A method for transforming correlated variables into a set of uncorrelated ones that better expose the various relationships among the original data items. 
  2. A method for identifying and ordering the dimensions along which data points exhibit the most variation. 
  3. Finding the best approximation of the original data points using fewer dimensions after discovering 2. 

###  1\. Example of Full Singular Value Decomposition

SVD is based on a theorem from linear algebra which says that a rectangular
matrix  $\displaystyle A$  can be broken down into the product of three
matrices - an orthogonal matrix  $\displaystyle U $  , a diagonal matrix
$\displaystyle S$  , and the transpose of an orthogonal matrix  $\displaystyle
V$  :  $\displaystyle A_{mn} = U_{mn}S_{mn}V_{nm}^T$

where  $\displaystyle U^TU = I, V^TV = I$  ; the columns of  $\displaystyle U$
are orthonormal eigenvectors of  $\displaystyle AA^T$  , the columns of V are
orthonormal eigenvectors of  $\displaystyle AA^T$  , and  $\displaystyle S$
is a diagonal matrix containing the square roots of eigenvalues from
$\displaystyle U$  or  $\displaystyle V$  in descending order.

computation steps:

  1. Because:  $\displaystyle A = U\sum V \Rightarrow A^T = V\sum U^T \Rightarrow A^TA = V{\sum}^2 U^T$  We can see that the square root of eigenvalues of matrix  $\displaystyle A^TA$  is the singular values of  $\displaystyle A$  . For  $\displaystyle \sum$  , we put the largest eigenvalue to the first row, and second largest to the second row, etc. 
  2. Get eigenvalues of matrix  $\displaystyle A^TA$  , which is a  $\displaystyle n$  by  $\displaystyle n$  square matrix, and we have  $\displaystyle n$  eigenvectors,  $\displaystyle V = [\vec{v_1}, \vec{v_2}, ..., \vec{v_n}]$  , we call each  $\displaystyle \vec{v_i}$  as the right singular vector of matrix A, each right singular vector is a column vector in  $\displaystyle V$  .  $\displaystyle \vec{v_i}$  corresponding to the i-th row in  $\displaystyle \sum$  . We need to apply the Gram-Schmidt orthonormalization process to the column vectors. 
  3. Get eigenvalues of matrix  $\displaystyle AA^T$  , which is a  $\displaystyle m$  by  $\displaystyle m$  square matrix, and we have  $\displaystyle m$  eigenvectors,  $\displaystyle Y = [\vec{u_1}, \vec{u_2}, ..., \vec{u_n}]$  , we call each  $\displaystyle \vec{u_i}$  as the left singular vector of matrix A. 

###  SVD in Dimension reduction

We could choose top  $\displaystyle k$  singular value to approximate the
matrix:

$\displaystyle A_{m\times n} = U_{m\times m}\sum_{m\times n}V^T_{n \times n}
\approx U_{m\times k} \sum_{k \times k}V^T_{k \times n} = \tilde{A}$

![](/img/in_post/svd.jpg)

If  $\displaystyle A$  is a matrix that consisting of variant of a
word*document matrix. SVD breaks it down into linearly independent components(
$\displaystyle U$  and  $\displaystyle V^T$  ). Then  $\displaystyle
U_{m\times k} \sum_{k \times k}V^T_{k \times n} $

The purpose is not to actually reconstruct the original matrix but to use the
reduced dimensionality representation to identify similar words and documents.
Documents are now represented by row vectors in  $\displaystyle V$  , and
document similarity is obtained by comparing rows in the matrix
$\displaystyle VS$  . Words are represented by row vectors in U, and word
similarity can be measured by computing row similarity in  $\displaystyle US$
.

### References

- $^{[1]}$[eigenvector/eigenvalue explanation](https://zhuanlan.zhihu.com/p/25955676)

- $^{[2]}$[SVD Tutorial](https://davetang.org/file/Singular_Value_Decomposition_Tutorial.pdf)
