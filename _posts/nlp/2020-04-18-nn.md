---
layout: post
title: "cs224n: Neural Network"
comments: true
author: "Yi"
header-style: text
mathjax: true
tags:
  - nlp
---

Notes for [Lecture 3 & 4](http://web.stanford.edu/class/cs224n/slides/cs224n-2020-lecture04-neuralnets.pdf).

##  Name Entity Recognition on Word Sequences

Goal: Train softmax classifier to classify a center word by taking
concatenation of word vectors surrounding it in a window.

We construct the sentence vector by concatenating all word vectors into
$\displaystyle X_{window}$  . As the figure below shows, for each class,
we apply a non-linear function  $\displaystyle f_c$  on the second layer to
learn non-linear interactions between the input word vectors, and finally do a
dot product to provide the final prediction  $\displaystyle s_c$  , the score
that the center word is a location. We can add a softmax over the score to get
a probability for each class.
![](/img/in_post/nn-cover.jpg)

When doing gradient descent, we'd like to compute the derivative of
$\displaystyle \frac{\partial \vec s}{\partial W}$  to train the parameter
$\displaystyle W$  . Notice that in logistic regression we only have 1 class,
but here we have  $\displaystyle c$  classes, so the dimension of
$\displaystyle w$  and  $\displaystyle b$  would be changed.

For this task,  $\displaystyle X \in R^{mk}$  is a matrix, where
$\displaystyle k$  is the window size and  $\displaystyle m$  is the word
vector dimension. So  $\displaystyle W \in R^{mk, c}, b \in R^c$  , where
$\displaystyle c$  is the class size.

$\displaystyle \frac{\partial\vec s}{\partial b} = \frac{\partial s}{\partial
a}\frac{\partial a}{\partial z}\frac{\partial z}{\partial
b}=h^Tdiag(f'(z))I=h^T\circ f' (z)$

We may reuse the intermediate steps above in  $\displaystyle \frac{\vec
s}{\partial W}$  .

$\displaystyle \frac{\partial\vec s} {\partial W} = u^T\circ
f'(z)\frac{\partial z}{\partial W} =\delta \frac{\partial z}{\partial W}$

We call  $\displaystyle \delta$  local error signal at z:  $\displaystyle
\frac{\partial s}{\partial z}$  ,  $\displaystyle x$  is local input signal.
Then

$\displaystyle \frac{\partial\vec s}{\partial W} = \delta^Tx^T$  (transpose is
added to let the result is the same matrix size as  $\displaystyle W$  ).

The size of the local error signal has the same dimensionality as the local
input.

##  Back Propagation

![](/img/in_post/v2-c1416c2b15fc9dc9307265125439cb3e_b.jpg)

downstream gradient = upstream gradient * local gradient. The example in the
lecture slides very clearly shows this idea.

For gradients that come from multiple places, we just sum them up.

Tricks to avoid overfitting(since we have so many parameters): regularization.
e.g L2 norm:

$\displaystyle J(\theta) = \frac{1}{N}\sum-
log(L(\theta))+\lambda\sum_k\theta_k^2$

