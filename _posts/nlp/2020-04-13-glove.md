---
layout: post
title: "cs224n: GloVe"
comments: true
author: "Yi"
header-style: text
mathjax: true
tags:
  - nlp
---

##  Motivation

GloVe(global vector) is a global log-bilinear regression model that combines
$^{[1]}$  :

  * The advantage of global matrix factorization (do poorly on the word analogy task, indicating a sub-optimal vector space structure) 
    * Efficient usage of global statistics 
    * Fast training 
  * The advantage of local context window methods (poorly utilize global statistics because it trains on separate local context window) 
    * capture complex pattern 
    * generally improve the performance 
    * scale with corpus size 

##  How does the objective function come

Let  $\displaystyle P_{ij} = P(j|i)=\frac{\\#word_j\text{ when } word_i \text{
is the center word}}{\\#word_i is the center word}$  . The following slides
$^{[2]}$  show the probability counted from the co-occurrence matrix. If we are
interested in the concept of thermodynamic phase, then we want to compare word
$\displaystyle ice$  and  $\displaystyle steam$  .

For word  $\displaystyle solid$  , it should be related to  $\displaystyle
ice$  but not  $\displaystyle steam$  . we see that  $\displaystyle
\frac{P_{steam, solid}}{P_{ice, solid}}$  is very large. For word
$\displaystyle gas$  , it should be related to  $\displaystyle steam$  but not
$\displaystyle ice$  . we see that  $\displaystyle \frac{P_{steam,
gas}}{P_{ice, gas}}$  is very small.

For word  $\displaystyle water$  , it should be either related to
$\displaystyle ice$  or  $\displaystyle steam$  . we see that  $\displaystyle
\frac{P_{steam, solid}}{P_{gas, solid}}$  is ~1.

> Compared to the raw probabilities, the ratio is better able to distinguish
> relevant words (solid and gas) from irrelevant words (water and fashion) and
> it is also better able to **discriminate** between the two relevant words

![](/img/in_post/glove-ratio.jpg)

Then how to find an objective function to mimic  $\displaystyle
\frac{P_{ik}}{P_{jk}}$  ? Since it related to 3 words, we represent
$\displaystyle F(w_i, w_j, \tilde w_k)$  ,  $\displaystyle w \in R^d$  are
word vectors and  $\displaystyle \tilde w \in R^d$  are separate context word
vectors. Since vector space are inherently linear structures, we'd like to use
vector differences to present the ratio  $\displaystyle \frac{P_{ik}}{P_{jk}}$
, and use a simple dot product to avoid obfuscating the linear structure. Then
$\displaystyle F((w_i - w_j)^T\tilde w_k) = \frac{P_{ik}}{P_{jk}}$  .

It is easy to think of exp function to convert  $\displaystyle e^{a-b}$  to
$\displaystyle \frac{e^a}{e^b}$  . So we can choose  $\displaystyle P_{ik} =
e^{w_i^T\tilde w_k}=\frac{X_{ik}}{X_i}, P_{jk}=e^{w_j^T\tilde w_k} =
\frac{X_{jk}}{X_i}$  Then  $\displaystyle \frac{P_{ik}}{P_{jk}} = e^{(w_i-
w_j)^T\tilde w_k} $

##  Improve the Simple Objective Function

  1. We place log on both side and got  $\displaystyle w_i^T\tilde w_k = log(X_{ik})-log(X_i)$  , this break the symmetry of  $\displaystyle w_i^T\tilde w_k$  because of  $\displaystyle log(X_i)$  . We can replace it with a bias term  $\displaystyle b_i$  for  $\displaystyle w_i$  and a bias term  $\displaystyle \tilde b_k$  for  $\displaystyle \tilde w_k$  . Then  $\displaystyle w_i^Tw_k + b_i + \tilde b_k = log(X_{ik})$  which is symmetric. 
  2. As we mentioned in logistic regression,  $\displaystyle log$  function will diverge whenever its argument is zero. When doing logistic regression we use sigmoid function -> $\displaystyle 1+e^x$  to resolve this issue. Here since  $\displaystyle X_{ik} > 0$  , we can just use  $\displaystyle 1+X_{ik}$  . 
  3. It weights all co-occurrences equally, e.g. if  $\displaystyle X_i = 1$  and  $\displaystyle X_{ik} = 1$  , this sample is too noisy. This issue is resolved by a weighting function  $\displaystyle f(X_{ik})$  :  $\displaystyle J = \sum_{i,k=1}^Vf(X_{ij})(w+i^Tw_j+b_i+\tilde b_j-logX_{ij})^2$  as long as: 

  * limiting  $\displaystyle J(x\to0)$  is not infinity like log function, 
  * $\displaystyle f(X_{ij})$  is non-decreasing so that it have smaller weight for low co-occurrence samples. 
  * $\displaystyle f(X_{ij})$  is not increasing too rapid so that frequency co-occurrences are not overweighted. 

The final weight function is shown as below:

![](/img/in_post/glove-fomula.jpg)

From the word embedding figure[3] we see that word vector trained by GloVe has
linear substructures, which meet the objective function's assumption.
![](/img/in_post/glove-cover.jpg)

### Reference
- $^{[1]}$[glove paper](https://nlp.stanford.edu/pubs/glove.pdf)
- $^{[2]}$[cs224n lecture02](http://web.stanford.edu/class/cs224n/slides/cs224n-2020-lecture02-wordvecs2.pdf)
- $^{[3]}$[glove official project website](https://nlp.stanford.edu/projects/glove/)
